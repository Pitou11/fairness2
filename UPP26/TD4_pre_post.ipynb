{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pitou11/fairness2/blob/main/UPP26/TD4_pre_post.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRe6S30b9Ng7"
      },
      "source": [
        "# TD 4: Mitigation des biais avec des méthodes de pré-processing et de post-processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JfRVXvrAVkM"
      },
      "source": [
        "## Installation of the environnement\n",
        "\n",
        "We highly recommend you to follow these steps, it will allow every student to work in an environment as similar as possible to the one used during testing.\n",
        "\n",
        "### Colab Settings ---- for Colab Users ONLY\n",
        "  The next cell of code are to execute only once per colab environment\n",
        "\n",
        "\n",
        "#### Python env creation (Colab only)\n",
        "\n",
        "        ```\n",
        "        ! python -m pip install numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit\n",
        "        ```\n",
        "#### 2. Download MEPS dataset (for part2) it can take several minutes (Colab only)\n",
        "\n",
        "        ```\n",
        "        ! Rscript /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/generate_data.R\n",
        "        ! mv h181.csv /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/\n",
        "        ! mv h192.csv /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/\n",
        "        ```\n",
        "\n",
        "### Local Settings ---- for installation on local computer ONLY\n",
        "\n",
        "If you arleady have an env from TD2 or TD3, you can simply reuse it.\n",
        "\n",
        "\n",
        "#### 1. Uv installation (local only, no need to redo if already done)\n",
        "\n",
        "\n",
        "        https://docs.astral.sh/uv/getting-started/installation/\n",
        "\n",
        "\n",
        "        `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
        "\n",
        "        Python version 3.12 installation (highly recommended)\n",
        "        `uv python install 3.12`\n",
        "\n",
        "#### 2. R installation *NEW* (local only)\n",
        "\n",
        "        In the command `Rscript` says 'command not found'\n",
        "\n",
        "        `sudo apt install r-base-core`\n",
        "\n",
        "#### 3. Python env creation (local only, no need to redo if already done)\n",
        "\n",
        "        ```\n",
        "        mkdir TD_bias_mitigation\n",
        "        cd TD_bias_mitigation\n",
        "        uv python pin 3.12\n",
        "        uv init\n",
        "        uv venv\n",
        "        uv add numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit\n",
        "        uv add pandas==2.2.2\n",
        "        ```\n",
        "\n",
        "#### 4. Download MEPS dataset, it can take several minutes *NEW* (local only)\n",
        "\n",
        "        ```\n",
        "        cd TD_bias_mitigation/.venv/lib/python3.12/site-packages/aif360/data/raw/meps/\n",
        "        Rscript generate_data.R\n",
        "        ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5RovVT0ZAVkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e4d59271-f5de-4222-c269-788254b335a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting fairlearn\n",
            "  Downloading fairlearn-0.13.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (5.10.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (6.17.1)\n",
            "Collecting causal-learn\n",
            "  Downloading causal_learn-0.1.4.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting BlackBoxAuditing\n",
            "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.12/dist-packages (1.6.7)\n",
            "Collecting dice-ml\n",
            "  Downloading dice_ml-0.12-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting shapkit\n",
            "  Downloading shapkit-0.0.4-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting aif360[inFairness]\n",
            "  Downloading aif360-0.6.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: narwhals>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (2.16.0)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (1.6.1)\n",
            "Collecting scipy<1.16.0,>=1.9.3 (from fairlearn)\n",
            "  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (26.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat) (4.26.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.9.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (6.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from aif360[inFairness]) (3.10.0)\n",
            "Collecting skorch (from aif360[inFairness])\n",
            "  Downloading skorch-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting inFairness>=0.2.2 (from aif360[inFairness])\n",
            "  Downloading inFairness-0.2.3-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: tensorflow>=1.13.1 in /usr/local/lib/python3.12/dist-packages (from aif360[AdversarialDebiasing]) (2.19.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from causal-learn) (0.21)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from causal-learn) (0.14.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from causal-learn) (3.6.1)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.12/dist-packages (from causal-learn) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from causal-learn) (4.67.2)\n",
            "Collecting momentchi2 (from causal-learn)\n",
            "  Downloading momentchi2-0.1.8-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (1.1.0)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (0.11.1)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (3.2.11)\n",
            "Collecting raiutils>=0.4.0 (from dice-ml)\n",
            "  Downloading raiutils-0.4.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (from dice-ml) (3.1.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from dice-ml) (4.6.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from shapkit) (0.13.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from clarabel>=0.5.0->cvxpy) (2.0.0)\n",
            "Collecting POT>=0.8.0 (from inFairness>=0.2.2->aif360[inFairness])\n",
            "  Downloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from inFairness>=0.2.2->aif360[inFairness]) (2.9.0+cpu)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.30.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.5.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (3.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy) (3.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy) (1.5.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2025.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from raiutils>=0.4.0->dice-ml) (2.32.4)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2026.1.28)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (2.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.5.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from skorch->aif360[inFairness]) (0.9.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels->causal-learn) (1.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost->dice-ml) (2.29.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.46.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.18.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.10.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->inFairness>=0.2.2->aif360[inFairness]) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->inFairness>=0.2.2->aif360[inFairness]) (1.14.0)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->inFairness>=0.2.2->aif360[inFairness]) (2025.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->clarabel>=0.5.0->cvxpy) (3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->osqp>=0.6.2->cvxpy) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->inFairness>=0.2.2->aif360[inFairness]) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (4.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.1.2)\n",
            "Downloading fairlearn-0.13.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading causal_learn-0.1.4.4-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.8/191.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dice_ml-0.12-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shapkit-0.0.4-py3-none-any.whl (20 kB)\n",
            "Downloading inFairness-0.2.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading raiutils-0.4.2-py3-none-any.whl (17 kB)\n",
            "Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aif360-0.6.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading momentchi2-0.1.8-py3-none-any.whl (11 kB)\n",
            "Downloading skorch-1.3.1-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: BlackBoxAuditing, lime\n",
            "  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394756 sha256=9922560adf743e268bc9d4c5c4e6aecdb69805b9c3fee04c463682d01efc3833\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/b9/e9/152a447c23f9c30559676a9f4ecee5c7e9a36130d48176555c\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=359274c9941d7bcb89abd6e63b8147c2a8afe7cb53ff1d6713aaa4ea296c7f6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/5d/0e/4b4fff9a47468fed5633211fb3b76d1db43fe806a17fb7486a\n",
            "Successfully built BlackBoxAuditing lime\n",
            "Installing collected packages: scipy, jedi, POT, momentchi2, skorch, raiutils, lime, inFairness, fairlearn, BlackBoxAuditing, aif360, shapkit, dice-ml, causal-learn\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "Successfully installed BlackBoxAuditing-0.1.54 POT-0.9.6.post1 aif360-0.6.1 causal-learn-0.1.4.4 dice-ml-0.12 fairlearn-0.13.0 inFairness-0.2.3 jedi-0.19.2 lime-0.2.0.1 momentchi2-0.1.8 raiutils-0.4.2 scipy-1.15.3 shapkit-0.0.4 skorch-1.3.1\n"
          ]
        }
      ],
      "source": [
        "# To execute only in Colab\n",
        "! python -m pip install numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNYIhwRfAaeS",
        "outputId": "af4ef2e1-9386-46e6-9baa-0730357e567d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "By using this script you acknowledge the responsibility for reading and\n",
            "abiding by any copyright/usage rules and restrictions as stated on the\n",
            "MEPS web site (https://meps.ahrq.gov/data_stats/data_use.jsp).\n",
            "\n",
            "Continue [y/n]? > y\n",
            "Loading required package: foreign\n",
            "trying URL 'https://meps.ahrq.gov/mepsweb/data_files/pufs/h181ssp.zip'\n",
            "Content type 'application/zip' length 13303652 bytes (12.7 MB)\n",
            "==================================================\n",
            "downloaded 12.7 MB\n",
            "\n",
            "Loading dataframe from file: h181.ssp\n",
            "Exporting dataframe to file: h181.csv\n",
            "trying URL 'https://meps.ahrq.gov/mepsweb/data_files/pufs/h192ssp.zip'\n",
            "Content type 'application/zip' length 15505898 bytes (14.8 MB)\n",
            "==================================================\n",
            "downloaded 14.8 MB\n",
            "\n",
            "Loading dataframe from file: h192.ssp\n",
            "Exporting dataframe to file: h192.csv\n"
          ]
        }
      ],
      "source": [
        "# To execute only in Colab\n",
        "! Rscript /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/generate_data.R\n",
        "! mv h181.csv /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/\n",
        "! mv h192.csv /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c9ITNS89NhA"
      },
      "source": [
        "## 1.Manipulate the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "88mMZIic9NhB"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "warnings.simplefilter(action=\"ignore\", append=True, category=UserWarning)\n",
        "# Datasets\n",
        "from aif360.datasets import MEPSDataset19\n",
        "from aif360.explainers import MetricTextExplainer\n",
        "\n",
        "# Fairness metrics\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "\n",
        "\n",
        "MEPSDataset19_data = MEPSDataset19()\n",
        "(dataset_orig_panel19_train, dataset_orig_panel19_val, dataset_orig_panel19_test) = (\n",
        "    MEPSDataset19().split([0.5, 0.8], shuffle=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LYbdfIPs9NhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f751ba-72cf-4a3d-e4ff-3f8f4348b192"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7915, 4749, 3166)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(dataset_orig_panel19_train.instance_weights), len(\n",
        "    dataset_orig_panel19_val.instance_weights\n",
        "), len(dataset_orig_panel19_test.instance_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t93y95AHAVkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a99ca1-1159-40c1-f3db-25d9c6d63de0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21854.981705, 18169.604822, 17191.832515, ...,  3896.116219,\n",
              "        4883.851005,  6630.588948])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "instance_weights = MEPSDataset19_data.instance_weights\n",
        "instance_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ABeNqPXrAVkN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "86a06368-ba0a-42c3-e3f4-d471da86f460"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Taille du dataset 15830, poids total du dataset 141367240.546316.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "f\"Taille du dataset {len(instance_weights)}, poids total du dataset {instance_weights.sum()}.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0oFO7CzAVkN"
      },
      "source": [
        "Conversion en dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-9_8B7pGAVkN"
      },
      "outputs": [],
      "source": [
        "def get_df(MepsDataset):\n",
        "    data = MepsDataset.convert_to_dataframe()\n",
        "    # data_train est un tuple, avec le data_frame et un dictionnaire avec toutes les infos (poids, attributs sensibles etc)\n",
        "    df = data[0]\n",
        "    df[\"WEIGHT\"] = data[1][\"instance_weights\"]\n",
        "    return df\n",
        "\n",
        "\n",
        "df = get_df(MEPSDataset19_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4WMnAYZAVkN"
      },
      "source": [
        "Nous réalisons maintenant l'opération inverse (qui sera indispensable pour le projet). Créer un objet de la classe StandardDataset de AIF360 à partir du dataframe.\n",
        "\n",
        "Pour le projet cela vous permettre d'utiliser les méthode déjà implémentées dans AIF360 sur votre jeu de données.\n",
        "\n",
        "Ici cela n'a aucun intéret car le dataframe vien d'un StandardDataset, nous vous fournissons le code. Mais cela vaut le coup de le lire attentivement et de poser des questions si besoin.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JkJksoOjAVkN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from aif360.datasets import StandardDataset\n",
        "import pandas as pd\n",
        "\n",
        "# Get categorical column from one hot encoding (specitic to MEPSdataset)\n",
        "# Here we create a dictionnary that links each categorical column name\n",
        "# to the list of corresponding one hot encoded columns\n",
        "categorical_columns_dic = {}\n",
        "for col in df.columns:\n",
        "    col_split = col.split(\"=\")\n",
        "    if len(col_split) > 1:\n",
        "        cat_col = col_split[0]\n",
        "        if not (cat_col in categorical_columns_dic.keys()):\n",
        "            categorical_columns_dic[cat_col] = []\n",
        "        categorical_columns_dic[cat_col].append(col)\n",
        "categorical_features = categorical_columns_dic.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tjYEWs-aAVkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c12739e-5540-4c79-8c7c-e51b83cd1187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15830, 140)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15830, 43)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Now we recreate the categorical column value from the one hot encoded\n",
        "print(df.shape)\n",
        "\n",
        "\n",
        "def categorical_transform(df, onehotencoded, cat_col):\n",
        "    if len(onehotencoded) > 1:\n",
        "        return df[onehotencoded].apply(\n",
        "            lambda x: onehotencoded[np.argmax(x)][len(cat_col) + 1 :], axis=1\n",
        "        )\n",
        "    else:\n",
        "        return df[onehotencoded]\n",
        "\n",
        "\n",
        "# Reverse the categorical one hot encoded\n",
        "for cat_col, onehotencoded in categorical_columns_dic.items():\n",
        "    df[cat_col] = categorical_transform(df, onehotencoded, cat_col)\n",
        "    df.drop(columns=onehotencoded, inplace=True)\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kfFSxUS0AVkN"
      },
      "outputs": [],
      "source": [
        "MyDataset = StandardDataset(\n",
        "    df=df,\n",
        "    label_name=\"UTILIZATION\",\n",
        "    favorable_classes=[1],\n",
        "    protected_attribute_names=[\"RACE\"],\n",
        "    privileged_classes=[[1]],\n",
        "    instance_weights_name=\"WEIGHT\",\n",
        "    categorical_features=categorical_features,\n",
        "    features_to_keep=[],\n",
        "    features_to_drop=[],\n",
        "    na_values=[\"?\", \"Unknown/Invalid\"],\n",
        "    custom_preprocessing=None,\n",
        "    metadata=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qTR8bsW8AVkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d35b35-a368-4fac-92b6-008c760fb712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.49826823461176517 0.21507139363038463\n",
            "0.49826823461176517 0.21507139363038463\n"
          ]
        }
      ],
      "source": [
        "# We check the dataset has the same metrics :D\n",
        "# Attention étonnanement le positive label 'favorable_classes' est par défaut 1 (cela est un peu bizarre pour ce dataset)\n",
        "print(\n",
        "    BinaryLabelDatasetMetric(\n",
        "        MEPSDataset19_data,\n",
        "        unprivileged_groups=[{\"RACE\": 0}],\n",
        "        privileged_groups=[{\"RACE\": 1}],\n",
        "    ).disparate_impact(),\n",
        "    BinaryLabelDatasetMetric(\n",
        "        MEPSDataset19_data,\n",
        "        unprivileged_groups=[{\"RACE\": 0}],\n",
        "        privileged_groups=[{\"RACE\": 1}],\n",
        "    ).base_rate(),\n",
        ")\n",
        "print(\n",
        "    BinaryLabelDatasetMetric(\n",
        "        MyDataset, unprivileged_groups=[{\"RACE\": 0}], privileged_groups=[{\"RACE\": 1}]\n",
        "    ).disparate_impact(),\n",
        "    BinaryLabelDatasetMetric(\n",
        "        MyDataset, unprivileged_groups=[{\"RACE\": 0}], privileged_groups=[{\"RACE\": 1}]\n",
        "    ).base_rate(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oQ28otZ9AVkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad052d5-bcb3-4b6a-8dd5-fe0899ed6ed5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4982682346117653, np.float64(0.21507139363038463))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from aif360.sklearn.metrics import disparate_impact_ratio, base_rate\n",
        "\n",
        "dir = disparate_impact_ratio(\n",
        "    y_true=df.UTILIZATION, prot_attr=df.RACE, pos_label=1, sample_weight=df.WEIGHT\n",
        ")\n",
        "br = base_rate(y_true=df.UTILIZATION, pos_label=1, sample_weight=df.WEIGHT)\n",
        "dir, br"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Zbw-U4mTKf"
      },
      "source": [
        "## 2. Appliquer les méthodes de pré-processing disponibles dans AIF360"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V2G9GokUAVkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3be4b5ba-7ff5-483a-e56a-853e8ef06d39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('RACE', [{'RACE': np.float64(0.0)}], [{'RACE': np.float64(1.0)}])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "sens_ind = 0\n",
        "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
        "unprivileged_groups = [\n",
        "    {sens_attr: v}\n",
        "    for v in dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]\n",
        "]\n",
        "privileged_groups = [\n",
        "    {sens_attr: v}\n",
        "    for v in dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]\n",
        "]\n",
        "sens_attr, unprivileged_groups, privileged_groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibSJ6X7DAVkN"
      },
      "source": [
        "### 2.1 Quesiton: Apprendre une regression logistique qui prédit l'UTILIZATION\n",
        "\n",
        "Attention nous avons enlever le preprocessing sur le dataframe, il faut cette fois utiliser l'API d'AIF360\n",
        "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StructuredDataset.html\n",
        "\n",
        "pour retrouver les features (X), les labels (y) et les poids de chaque instance du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MP8OBN2CAVkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe150a99-fa3c-45d3-98ac-7e56698dedc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7915, 138) (7915,) (7915,)\n",
            "(4749, 138) (4749,) (4749,)\n"
          ]
        }
      ],
      "source": [
        "#training set\n",
        "X_train = dataset_orig_panel19_train.features\n",
        "y_train = dataset_orig_panel19_train.labels.ravel()\n",
        "w_train = dataset_orig_panel19_train.instance_weights.ravel()\n",
        "\n",
        "#validation set\n",
        "X_val = dataset_orig_panel19_val.features\n",
        "y_val = dataset_orig_panel19_val.labels.ravel()\n",
        "w_val = dataset_orig_panel19_val.instance_weights.ravel()\n",
        "\n",
        "\n",
        "print(X_train.shape, y_train.shape, w_train.shape)\n",
        "print(X_val.shape, y_val.shape, w_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz0SSilCAVkN"
      },
      "source": [
        "### 2.2 Question: Calcul des métriques de fairness\n",
        "\n",
        "Calculer les métriques du dataset de validation seul.\n",
        "\n",
        "Calculer les métriques basées sur les prédictions et la vérité du dataset de validation.\n",
        "\n",
        "En comparaison calculer les métriques basées sur des prédictions aléatoires et la vérité du dataset de validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4DG8_NX_AVkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3464f28-b57a-4257-b2a6-938c186959e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics on validation dataset\n",
            "  Disparate Impact: 0.4677\n",
            "  Base Rate (privileged): 0.2877\n",
            "  Base Rate (unprivileged): 0.1346\n",
            "  Base Rate (overall): 0.2279\n"
          ]
        }
      ],
      "source": [
        "print(\"Metrics on validation dataset\")\n",
        "\n",
        "metric_val = BinaryLabelDatasetMetric(\n",
        "    dataset_orig_panel19_val,\n",
        "    unprivileged_groups=unprivileged_groups,\n",
        "    privileged_groups=privileged_groups\n",
        ")\n",
        "\n",
        "print(f\"  Disparate Impact: {metric_val.disparate_impact():.4f}\")\n",
        "print(f\"  Base Rate (privileged): {metric_val.base_rate(privileged=True):.4f}\")\n",
        "print(f\"  Base Rate (unprivileged): {metric_val.base_rate(privileged=False):.4f}\")\n",
        "print(f\"  Base Rate (overall): {metric_val.base_rate():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ydyQtGxDAVkO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "ad21f959-4628-48cd-9812-2470b8ced76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics based on predictions and truth on validation dataset\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset_orig_panel19' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1594281718.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Metrics based on predictions and truth on validation dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetric_basedonpred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationMetric\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdataset_orig_panel19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_orig_panel19_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Accuracy: {metric_basedonpred_val.accuracy():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_orig_panel19' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Metrics based on predictions and truth on validation dataset\")\n",
        "metric_basedonpred_val = ClassificationMetric( )\n",
        "\n",
        "\n",
        "print(f\"  Accuracy: {metric_basedonpred_val.accuracy():.4f}\")\n",
        "print(f\"  Balanced Accuracy: {metric_basedonpred_val.balanced_accuracy():.4f}\")\n",
        "print(f\"  Disparate Impact: {metric_basedonpred_val.disparate_impact():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67764ce5",
        "outputId": "1ba3db4c-282b-43f8-f48c-771d2a8ca751"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "# We use solver='liblinear' for better compatibility with small datasets or weighted samples\n",
        "model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "model.fit(X_train, y_train, sample_weight=w_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred_val = model.predict(X_val)\n",
        "y_prob_val = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"Logistic Regression model trained and predictions made on validation set.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained and predictions made on validation set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7924862"
      },
      "source": [
        "Maintenant que nous avons les prédictions, nous pouvons créer le `dataset_pred_val` nécessaire à `ClassificationMetric`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4adc86eb",
        "outputId": "cf6249c7-0011-4e73-bb19-3127dfcb61d5"
      },
      "source": [
        "from aif360.datasets import BinaryLabelDataset\n",
        "\n",
        "# Create a BinaryLabelDataset for predictions on the validation set\n",
        "dataset_pred_val = dataset_orig_panel19_val.copy(deepcopy=True)\n",
        "dataset_pred_val.labels = y_pred_val.reshape(-1, 1)\n",
        "\n",
        "# Now you can use ClassificationMetric\n",
        "print(\"Metrics based on predictions and truth on validation dataset\")\n",
        "metric_basedonpred_val = ClassificationMetric(\n",
        "    dataset_orig_panel19_val, # Ground truth dataset\n",
        "    dataset_pred_val,       # Predicted labels dataset\n",
        "    unprivileged_groups=unprivileged_groups,\n",
        "    privileged_groups=privileged_groups\n",
        ")\n",
        "\n",
        "print(f\"  Accuracy: {metric_basedonpred_val.accuracy():.4f}\")\n",
        "print(f\"  Base Rate: {metric_basedonpred_val.base_rate():.4f}\")\n",
        "print(f\"  Disparate Impact: {metric_basedonpred_val.disparate_impact():.4f}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics based on predictions and truth on validation dataset\n",
            "  Accuracy: 0.8344\n",
            "  Base Rate: 0.2279\n",
            "  Disparate Impact: 0.3703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c1ae623",
        "outputId": "52d3cd2a-318a-45d5-edf4-b3d03cbec7cd"
      },
      "source": [
        "import numpy as np\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "\n",
        "# Generate random predictions for the validation set\n",
        "# We'll make sure the shape matches y_val\n",
        "y_pred_random = np.random.randint(0, 2, size=y_val.shape).reshape(-1, 1)\n",
        "\n",
        "# Create a BinaryLabelDataset for random predictions on the validation set\n",
        "dataset_pred_random = dataset_orig_panel19_val.copy(deepcopy=True)\n",
        "dataset_pred_random.labels = y_pred_random\n",
        "\n",
        "print(\"Metrics based on random predictions and truth on validation dataset\")\n",
        "metric_random_pred_val = ClassificationMetric(\n",
        "    dataset_orig_panel19_val, # Ground truth dataset\n",
        "    dataset_pred_random,      # Randomly predicted labels dataset\n",
        "    unprivileged_groups=unprivileged_groups,\n",
        "    privileged_groups=privileged_groups\n",
        ")\n",
        "\n",
        "print(f\"  Accuracy (Random): {metric_random_pred_val.accuracy():.4f}\")\n",
        "print(f\"  Base Rate (Random): {metric_random_pred_val.base_rate():.4f}\")\n",
        "print(f\"  Disparate Impact (Random): {metric_random_pred_val.disparate_impact():.4f}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics based on random predictions and truth on validation dataset\n",
            "  Accuracy (Random): 0.5050\n",
            "  Base Rate (Random): 0.2279\n",
            "  Disparate Impact (Random): 1.0752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Haix0kTKAVkO"
      },
      "source": [
        "### 2.2 Repondération\n",
        "#### 2.2.1. Question : Trouver dans l'API quels objets/fonctions sont à utiliser pour faire de repondération et les appliquer sur le dataset d'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "FrkxtMgvAVkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782328a2-b4b1-4faa-c9c4-d50fee723e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reweighing transformation applied to the training dataset.\n"
          ]
        }
      ],
      "source": [
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "model_reweighing = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "# Fit the reweighing model on the training dataset\n",
        "model_reweighing.fit(dataset_orig_panel19_train)\n",
        "\n",
        "# Transform the training dataset to get the reweighed version\n",
        "dataset_reweighed = model_reweighing.transform(dataset_orig_panel19_train)\n",
        "\n",
        "print(\"Reweighing transformation applied to the training dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSbOgCdYAVkO"
      },
      "source": [
        "#### 2.2.2. Question: Apprendre une regression logistique sur les données pondérées et calculer les métriques de fairness sur l'échantillon de validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8oTQuuqAVkO"
      },
      "source": [
        "Comme vu en cours le Reweighting ne modifie que la pondération du dataset, les features et label restent inchangés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42EmGC1yAVkO"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idpx9JQhAVkO"
      },
      "source": [
        "### 2.3. Disparate Impact Remover\n",
        "#### 2.3.1. Question : Trouver dans l'API quels objets/fonctions sont à utiliser pour faire une approache de disparate impact remover et les appliquer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXdACyilAVkO"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0974hphlAVkO"
      },
      "source": [
        "#### 2.3.2. Question: Apprendre une regression logistique sur les données transformées en retirant l'attribut sensible et calculer les métriques de fairness sur l'échantillon de validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTRjV4CNAVkO"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxu533OnAVkO"
      },
      "source": [
        "### 2.4. Question: Apprentissage de représentation latente fair\n",
        "\n",
        "Apprendre le pre-processing et evaluer son impact avec les métriques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u6H09m3AVkO"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3VAODJBAVkO"
      },
      "source": [
        "## 3 Post processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgnuqCJXAVkO"
      },
      "source": [
        "### 3.1 Question: Use the post-processing Reject Option Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcWJb5YEAVkO"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV79_o6GAVkO"
      },
      "source": [
        "#### 3.1.1 Reuse the first Logistic Regression learn to find the best threshold that maximises its balanced accuracy on the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo6fSOFUAVkU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHaEP8jeAVkU"
      },
      "source": [
        "#### 3.1.2 Use the RejectOptionClassification  on the validation dataset with the logistic regression predictions. To improve the fairness metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dBtrr-fAVkU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGeqsseWAVkU"
      },
      "source": [
        "#### 3.1.3 Do the same while starting from the Logistic Regression learned on the Reweighted dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daLpS6iHAVkU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yGdW4ZvAVkU"
      },
      "source": [
        "#### 3.2 Use the Calibrated Equalised Odds  on the validation dataset with the logistic regression predictions. To improve the fairness metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKzaffI2AVkU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "td3_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}