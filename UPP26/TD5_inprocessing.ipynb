{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pitou11/fairness2/blob/main/UPP26/TD5_inprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRe6S30b9Ng7"
      },
      "source": [
        "# TD 5: Mitigation des biais avec une méthode de in-processsing Prejudice Remover\n",
        "\n",
        "The aim of this notebook is to use the Prejudice Remover in-processing approach and analyse its impact on the model output.\n",
        "In terms of Machine Learning we will go a bit further in the train/valid/test paradigm.\n",
        "\n",
        "The model has to be learn on the train dataset, then the model parameters has to be optimized on the valid dataset, and finally the model performance is evaluated on the test dataset.\n",
        "No choice/decision etc can be taken depending on the test dataset. This could result on an overfitting on the test dataset.\n",
        "\n",
        "Here you will manipulate:\n",
        "- Prejudice Remover approach as a black box\n",
        "- Training of the prejudice remover using the train/valid paradigm. to choice the 'best' threshold\n",
        "- Combine Prejudice Remover with Reweighing\n",
        "\n",
        "As a reminder of pre-processing approach we encourage you to :\n",
        "- analyse the impact of the Reweighing on different model (Logistic Regression, Decision Tree, Random Forest, etc.)\n",
        "\n",
        "\n",
        "## Installation of the environnement\n",
        "\n",
        "We highly recommend you to follow these steps, it will allow every student to work in an environment as similar as possible to the one used during testing.\n",
        "\n",
        "### Colab Settings ---- for Colab Users ONLY\n",
        "  The next cell of code are to execute only once per colab environment\n",
        "\n",
        "\n",
        "#### Python env creation (Colab only)\n",
        "\n",
        "        ```\n",
        "        ! python -m pip install numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit\n",
        "        ```\n",
        "#### 2. Download MEPS dataset (for part2) it can take several minutes (Colab only)\n",
        "\n",
        "        ```\n",
        "        ! Rscript /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/generate_data.R\n",
        "        ! mv h181.csv /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/\n",
        "        ! mv h192.csv /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/\n",
        "        ```\n",
        "\n",
        "### Local Settings ---- for installation on local computer ONLY\n",
        "\n",
        "If you arleady have an env from TD2, TD3 or TD4, you can simply reuse it.\n",
        "\n",
        "\n",
        "#### 1. Uv installation (local only, no need to redo if already done)\n",
        "\n",
        "\n",
        "        https://docs.astral.sh/uv/getting-started/installation/\n",
        "\n",
        "\n",
        "        `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
        "\n",
        "        Python version 3.12 installation (highly recommended)\n",
        "        `uv python install 3.12`\n",
        "\n",
        "#### 2. R installation *NEW* (local only)\n",
        "\n",
        "        In the command `Rscript` says 'command not found'\n",
        "\n",
        "        `sudo apt install r-base-core`\n",
        "\n",
        "#### 3. Python env creation (local only, no need to redo if already done)\n",
        "\n",
        "        ```\n",
        "        mkdir TD_bias_mitigation\n",
        "        cd TD_bias_mitigation\n",
        "        uv python pin 3.12\n",
        "        uv init\n",
        "        uv venv\n",
        "        uv add numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit\n",
        "        uv add pandas==2.2.2\n",
        "        ```\n",
        "\n",
        "#### 4. Download MEPS dataset, it can take several minutes *NEW* (local only)\n",
        "\n",
        "        ```\n",
        "        cd TD_bias_mitigation/.venv/lib/python3.12/site-packages/aif360/data/raw/meps/\n",
        "        Rscript generate_data.R\n",
        "        ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XS-rsfZKD3SC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d3e93d-e8ae-46ca-c055-2e3bc2abf304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting fairlearn\n",
            "  Downloading fairlearn-0.13.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (5.10.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (6.17.1)\n",
            "Collecting causal-learn\n",
            "  Downloading causal_learn-0.1.4.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting BlackBoxAuditing\n",
            "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.12/dist-packages (1.6.7)\n",
            "Collecting dice-ml\n",
            "  Downloading dice_ml-0.12-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting shapkit\n",
            "  Downloading shapkit-0.0.4-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting aif360[inFairness]\n",
            "  Downloading aif360-0.6.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: narwhals>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (2.16.0)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from fairlearn) (1.6.1)\n",
            "Collecting scipy<1.16.0,>=1.9.3 (from fairlearn)\n",
            "  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (26.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat) (4.26.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.9.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (6.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from aif360[inFairness]) (3.10.0)\n",
            "Collecting skorch (from aif360[inFairness])\n",
            "  Downloading skorch-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting inFairness>=0.2.2 (from aif360[inFairness])\n",
            "  Downloading inFairness-0.2.3-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: tensorflow>=1.13.1 in /usr/local/lib/python3.12/dist-packages (from aif360[AdversarialDebiasing]) (2.19.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from causal-learn) (0.21)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from causal-learn) (0.14.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from causal-learn) (3.6.1)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.12/dist-packages (from causal-learn) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from causal-learn) (4.67.3)\n",
            "Collecting momentchi2 (from causal-learn)\n",
            "  Downloading momentchi2-0.1.8-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (1.1.1)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (0.11.1)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (3.2.11)\n",
            "Collecting raiutils>=0.4.0 (from dice-ml)\n",
            "  Downloading raiutils-0.4.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (from dice-ml) (3.2.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from dice-ml) (4.6.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from shapkit) (0.13.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from clarabel>=0.5.0->cvxpy) (2.0.0)\n",
            "Collecting POT>=0.8.0 (from inFairness>=0.2.2->aif360[inFairness])\n",
            "  Downloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from inFairness>=0.2.2->aif360[inFairness]) (2.10.0+cpu)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.30.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.9.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->aif360[inFairness]) (3.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy) (3.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy) (1.5.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.3->fairlearn) (2025.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from raiutils>=0.4.0->dice-ml) (2.32.4)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2026.2.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (5.29.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (1.78.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.5.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from skorch->aif360[inFairness]) (0.9.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels->causal-learn) (1.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost->dice-ml) (2.29.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.46.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.18.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (3.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->inFairness>=0.2.2->aif360[inFairness]) (3.24.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->inFairness>=0.2.2->aif360[inFairness]) (1.14.0)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->inFairness>=0.2.2->aif360[inFairness]) (2025.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->clarabel>=0.5.0->cvxpy) (3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->osqp>=0.6.2->cvxpy) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->inFairness>=0.2.2->aif360[inFairness]) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (4.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=1.13.1->aif360[AdversarialDebiasing]) (0.1.2)\n",
            "Downloading fairlearn-0.13.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading causal_learn-0.1.4.4-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.8/191.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dice_ml-0.12-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shapkit-0.0.4-py3-none-any.whl (20 kB)\n",
            "Downloading inFairness-0.2.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading raiutils-0.4.2-py3-none-any.whl (17 kB)\n",
            "Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aif360-0.6.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading momentchi2-0.1.8-py3-none-any.whl (11 kB)\n",
            "Downloading skorch-1.3.1-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: BlackBoxAuditing, lime\n",
            "  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394756 sha256=658c08d7de8aa571579fb04f2791af0f27fe8b1f7de7741832523b4fc4c0bac5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/b9/e9/152a447c23f9c30559676a9f4ecee5c7e9a36130d48176555c\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=e51feaa500fa981dcdf4e44211e1ba651a98ea108d6c92bf7c70bb46e8170be0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/5d/0e/4b4fff9a47468fed5633211fb3b76d1db43fe806a17fb7486a\n",
            "Successfully built BlackBoxAuditing lime\n",
            "Installing collected packages: scipy, jedi, POT, momentchi2, skorch, raiutils, lime, inFairness, fairlearn, BlackBoxAuditing, aif360, shapkit, dice-ml, causal-learn\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "Successfully installed BlackBoxAuditing-0.1.54 POT-0.9.6.post1 aif360-0.6.1 causal-learn-0.1.4.4 dice-ml-0.12 fairlearn-0.13.0 inFairness-0.2.3 jedi-0.19.2 lime-0.2.0.1 momentchi2-0.1.8 raiutils-0.4.2 scipy-1.15.3 shapkit-0.0.4 skorch-1.3.1\n"
          ]
        }
      ],
      "source": [
        "# To execute only in Colab\n",
        "! python -m pip install numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3av_Ex8V8le",
        "outputId": "96f4e6d3-08bd-4f75-a788-6d723453ec29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "By using this script you acknowledge the responsibility for reading and\n",
            "abiding by any copyright/usage rules and restrictions as stated on the\n",
            "MEPS web site (https://meps.ahrq.gov/data_stats/data_use.jsp).\n",
            "\n",
            "Continue [y/n]? > y\n",
            "Loading required package: foreign\n",
            "trying URL 'https://meps.ahrq.gov/mepsweb/data_files/pufs/h181ssp.zip'\n",
            "Content type 'application/zip' length 13303652 bytes (12.7 MB)\n",
            "==================================================\n",
            "downloaded 12.7 MB\n",
            "\n",
            "Loading dataframe from file: h181.ssp\n",
            "Exporting dataframe to file: h181.csv\n",
            "trying URL 'https://meps.ahrq.gov/mepsweb/data_files/pufs/h192ssp.zip'\n",
            "Content type 'application/zip' length 15505898 bytes (14.8 MB)\n",
            "==================================================\n",
            "downloaded 14.8 MB\n",
            "\n",
            "Loading dataframe from file: h192.ssp\n",
            "Exporting dataframe to file: h192.csv\n"
          ]
        }
      ],
      "source": [
        "# To execute only in Colab\n",
        "! Rscript /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/generate_data.R\n",
        "! mv h181.csv /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/\n",
        "! mv h192.csv /usr/local/lib/python3.12/dist-packages/aif360/data/raw/meps/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c9ITNS89NhA"
      },
      "source": [
        "## 1. Import and load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "88mMZIic9NhB"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "warnings.simplefilter(action=\"ignore\", append=True, category=UserWarning)\n",
        "# Datasets\n",
        "from aif360.datasets import MEPSDataset19\n",
        "\n",
        "# Fairness metrics\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "MEPSDataset19_data = MEPSDataset19()\n",
        "(dataset_orig_panel19_train, dataset_orig_panel19_val, dataset_orig_panel19_test) = (\n",
        "    MEPSDataset19().split([0.5, 0.8], shuffle=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LYbdfIPs9NhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e06e470-d991-45b3-bc57-0ec10baeb3a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7915, 4749, 3166)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(dataset_orig_panel19_train.instance_weights), len(\n",
        "    dataset_orig_panel19_val.instance_weights\n",
        "), len(dataset_orig_panel19_test.instance_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z5zWlgrRDycU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b7383b-bf07-471c-ab07-f8bdc8a0cab6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21854.981705, 18169.604822, 17191.832515, ...,  3896.116219,\n",
              "        4883.851005,  6630.588948])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "instance_weights = MEPSDataset19_data.instance_weights\n",
        "instance_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M9zwCl6nDycU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1b626d6c-99d6-49a7-feff-530d390b757c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Taille du dataset 15830, poids total du dataset 141367240.546316.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "f\"Taille du dataset {len(instance_weights)}, poids total du dataset {instance_weights.sum()}.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cX6Vk9nIDycU"
      },
      "outputs": [],
      "source": [
        "from aif360.sklearn.metrics import *\n",
        "from sklearn.metrics import  balanced_accuracy_score\n",
        "\n",
        "\n",
        "# This method takes lists\n",
        "def get_metrics(\n",
        "    y_true, # list or np.array of truth values\n",
        "    y_pred=None,  # list or np.array of predictions\n",
        "    prot_attr=None, # list or np.array of protected/sensitive attribute values\n",
        "    priv_group=1, # value taken by the privileged group\n",
        "    pos_label=1, # value taken by the positive truth/prediction\n",
        "    sample_weight=None # list or np.array of weights value,\n",
        "):\n",
        "    group_metrics = {}\n",
        "    group_metrics[\"base_rate_truth\"] = base_rate(\n",
        "        y_true=y_true, pos_label=pos_label, sample_weight=sample_weight\n",
        "    )\n",
        "    group_metrics[\"statistical_parity_difference\"] = statistical_parity_difference(\n",
        "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
        "    )\n",
        "    group_metrics[\"disparate_impact_ratio\"] = disparate_impact_ratio(\n",
        "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
        "    )\n",
        "    if not y_pred is None:\n",
        "        group_metrics[\"base_rate_preds\"] = base_rate(\n",
        "        y_true=y_pred, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        group_metrics[\"equal_opportunity_difference\"] = equal_opportunity_difference(\n",
        "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        group_metrics[\"average_odds_difference\"] = average_odds_difference(\n",
        "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        if len(set(y_pred))>1:\n",
        "            group_metrics[\"conditional_demographic_disparity\"] = conditional_demographic_disparity(\n",
        "                y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
        "            )\n",
        "        else:\n",
        "            group_metrics[\"conditional_demographic_disparity\"] =None\n",
        "        group_metrics[\"smoothed_edf\"] = smoothed_edf(\n",
        "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        group_metrics[\"df_bias_amplification\"] = df_bias_amplification(\n",
        "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
        "        )\n",
        "        group_metrics[\"balanced_accuracy_score\"] = balanced_accuracy_score(\n",
        "        y_true=y_true, y_pred=y_pred, sample_weight=sample_weight\n",
        "        )\n",
        "    return group_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV0F7tkTDycU"
      },
      "source": [
        "## Learning a Prejudice Remover model on the training dataset, and choose the best parameters with the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_dWBZ5RmDycU"
      },
      "outputs": [],
      "source": [
        "# Bias mitigation techniques\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.algorithms.inprocessing import PrejudiceRemover"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlN5mT7VDycU"
      },
      "source": [
        "### Question1 : Learn a Standard Scaler on the training dataset features, its output will be used as input of the model learned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G5ZzO-yADycU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95ce743-2f91-4dd3-c477-64f3a5fa174c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.18960685  1.33092394  0.59754104 ...  0.9452728  -0.74601531\n",
            "  -0.35930667]\n",
            " [ 0.43824652  1.33092394  1.03995285 ...  0.9452728  -0.74601531\n",
            "  -0.35930667]\n",
            " [ 1.36639751  1.33092394 -1.14944794 ...  0.9452728  -0.74601531\n",
            "  -0.35930667]\n",
            " ...\n",
            " [-0.35731147  1.33092394 -1.14944794 ... -1.05789567  1.34045507\n",
            "  -0.35930667]\n",
            " [ 0.30565352 -0.75135774  1.16322906 ...  0.9452728  -0.74601531\n",
            "  -0.35930667]\n",
            " [-0.66669513  1.33092394  1.11484027 ... -1.05789567 -0.74601531\n",
            "   2.78313786]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_scale = scaler.fit_transform(dataset_orig_panel19_train.features)\n",
        "print(train_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0sqqKAcDycU"
      },
      "source": [
        "### Question2: Create a method to learn a Prejudice Remover on the train dataset and retrieve the model learned\n",
        "Execute the method with the parameter eta arbitrarily set at 25.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UbVz88N_DycU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652f35bc-17e1-4b37-fa2b-24caeecdafa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<aif360.algorithms.inprocessing.prejudice_remover.PrejudiceRemover object at 0x7829f77344d0>\n"
          ]
        }
      ],
      "source": [
        "pr = PrejudiceRemover(eta = 25.0,sensitive_attr=\"RACE\", class_attr=\"\")\n",
        "train_pr = pr.fit(dataset_orig_panel19_train)\n",
        "print(train_pr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2bKnNo_DycU"
      },
      "source": [
        "Le score du Prejudice Remover donne un sortie pour chaque instance une seule valeur, c'est un seuil, arbritrairement fixé à 0.5 par défault, qui permet à partir de ce score de décider la prédiction 1 ou 0.\n",
        "Si le score est supérieur au seuil la prédiction est 1, sinon c'est 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EZ13xMyDycU"
      },
      "source": [
        "### Validating: Choose the best parameters\n",
        "\n",
        "Here there are two parameters :\n",
        "- eta: fairness penalty parameter of the PR model\n",
        "- thershold: the threshold of the binary classification\n",
        "\n",
        "The threshold is used to obtains predictions from the model output.\n",
        "The eta is used during the training\n",
        "\n",
        "Question3: Create a method that will loop over 50 threshold ]0:0.5( and 5 values of ETA [1.0: 100.0], and outputs the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uAY1-DfzDycU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfc23b5-39f5-479e-aa56-022b5ead7830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation for 3 eta values and 10 thresholds.\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.01 (1/10)...\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.06 (2/10)...\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.12 (3/10)...\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.17 (4/10)...\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.23 (5/10)...\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.28 (6/10)...\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.34 (7/10)...\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.39 (8/10)...\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.45 (9/10)...\n",
            "  Evaluating eta=1.00 (1/3), threshold=0.50 (10/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.01 (1/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.06 (2/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.12 (3/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.17 (4/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.23 (5/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.28 (6/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.34 (7/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.39 (8/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.45 (9/10)...\n",
            "  Evaluating eta=50.50 (2/3), threshold=0.50 (10/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.01 (1/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.06 (2/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.12 (3/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.17 (4/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.23 (5/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.28 (6/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.34 (7/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.39 (8/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.45 (9/10)...\n",
            "  Evaluating eta=100.00 (3/3), threshold=0.50 (10/10)...\n",
            "   eta  threshold  base_rate_truth  statistical_parity_difference  \\\n",
            "0  1.0   0.010000         0.217475                      -0.062720   \n",
            "1  1.0   0.064444         0.217475                      -0.355727   \n",
            "2  1.0   0.118889         0.217475                      -0.339446   \n",
            "3  1.0   0.173333         0.217475                      -0.279138   \n",
            "4  1.0   0.227778         0.217475                      -0.234978   \n",
            "\n",
            "   disparate_impact_ratio  base_rate_preds  equal_opportunity_difference  \\\n",
            "0                0.937063         0.970903                     -0.001814   \n",
            "1                0.550292         0.645515                     -0.105723   \n",
            "2                0.429741         0.456405                     -0.177960   \n",
            "3                0.387600         0.341635                     -0.185738   \n",
            "4                0.366051         0.274545                     -0.174793   \n",
            "\n",
            "   average_odds_difference  conditional_demographic_disparity  smoothed_edf  \\\n",
            "0                -0.036796                          -0.097641      2.955907   \n",
            "1                -0.232066                          -0.068370      0.994062   \n",
            "2                -0.239579                          -0.060172      0.844573   \n",
            "3                -0.203268                          -0.054581      0.947780   \n",
            "4                -0.174624                          -0.051886      1.004982   \n",
            "\n",
            "   df_bias_amplification  balanced_accuracy_score  \n",
            "0               2.280326                 0.518290  \n",
            "1               0.318481                 0.687799  \n",
            "2               0.168992                 0.751561  \n",
            "3               0.272200                 0.761924  \n",
            "4               0.329402                 0.742748  \n"
          ]
        }
      ],
      "source": [
        "threshold = np.linspace(0.01, 0.5, 10) # Reduced to 10 values for faster execution, starting from 0.01\n",
        "eta = np.linspace(1.0, 100.0, 3) # Reduced to 3 values for faster execution\n",
        "\n",
        "# Define sensitive and class attributes\n",
        "sensitive_attribute = 'RACE'\n",
        "class_attribute = dataset_orig_panel19_train.label_names[0] # 'UTILIZATION'\n",
        "privileged_group = 1.0 # For 'RACE' == 1\n",
        "positive_label = 1.0 # For 'UTILIZATION' == 1\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"Starting evaluation for {len(eta)} eta values and {len(threshold)} thresholds.\")\n",
        "\n",
        "for i, current_eta in enumerate(eta):\n",
        "  for j, current_threshold in enumerate(threshold):\n",
        "    print(f\"  Evaluating eta={current_eta:.2f} ({i+1}/{len(eta)}), threshold={current_threshold:.2f} ({j+1}/{len(threshold)})...\")\n",
        "    # 1. Train Prejudice Remover model\n",
        "    pr = PrejudiceRemover(\n",
        "        eta=current_eta,\n",
        "        sensitive_attr=sensitive_attribute,\n",
        "        class_attr=class_attribute,\n",
        "    )\n",
        "    # Note: PrejudiceRemover's fit method does not return the fitted object itself.\n",
        "    # The object `pr` is modified in-place.\n",
        "    pr.fit(dataset_orig_panel19_train)\n",
        "\n",
        "    # 2. Get scores on validation dataset\n",
        "    # The predict method returns a Dataset object, from which we can extract scores.\n",
        "    val_scores_dataset = pr.predict(dataset_orig_panel19_val)\n",
        "    val_scores = val_scores_dataset.scores # scores are usually in the `scores` attribute of the returned Dataset\n",
        "\n",
        "    # 3. Make binary predictions based on threshold\n",
        "    val_pred = (val_scores > current_threshold).astype(float)\n",
        "    prot_attr_val = dataset_orig_panel19_val.protected_attributes[:, dataset_orig_panel19_val.protected_attribute_names.index(sensitive_attribute)]\n",
        "\n",
        "    # 5. Calculate metrics\n",
        "    metrics = get_metrics(\n",
        "        y_true=dataset_orig_panel19_val.labels[:, 0],\n",
        "        y_pred=val_pred[:, 0],\n",
        "        prot_attr=prot_attr_val,\n",
        "        priv_group=privileged_group,\n",
        "        pos_label=positive_label,\n",
        "        sample_weight=dataset_orig_panel19_val.instance_weights,\n",
        "    )\n",
        "    result_row = {\n",
        "        \"eta\": current_eta,\n",
        "        \"threshold\": current_threshold,\n",
        "    }\n",
        "    result_row.update(metrics)\n",
        "    results.append(result_row)\n",
        "\n",
        "# Convert results to a DataFrame for easier analysis\n",
        "evaluation_results = pd.DataFrame(results)\n",
        "\n",
        "print(evaluation_results.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3XbHo41DycU"
      },
      "source": [
        "### Question4 : Make plot to choose the best set of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqwWdgVVDycU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0K7A5u_DycU"
      },
      "source": [
        "### Question 5: Evaluate : compute the metrics on the test dataset using the model learnt with the selected parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVii7zd7DycU"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9vzL4RaDycU"
      },
      "source": [
        "## Combine pre-processing and in-processing\n",
        "### Question6: Redo the Prejudice Remover approach using first the Reweighing pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hyCLr_UqDycV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a070382-da83-431b-b521-b1675e50ee5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<aif360.algorithms.preprocessing.reweighing.Reweighing object at 0x7829f72eac60>\n"
          ]
        }
      ],
      "source": [
        "rw = Reweighing(unprivileged_groups=[{'RACE': 0.0}], privileged_groups=[{'RACE': 1.0}])\n",
        "train_rw = rw.fit(dataset_orig_panel19_train)\n",
        "print(train_rw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILgpsj0aDycV"
      },
      "source": [
        "## Adversarial Debiasing\n",
        "\n",
        "Adversarial debiasing [1] is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary's ability to determine the protected attribute from the predictions.\n",
        "\n",
        "See [AIF360 tuto](https://github.com/Trusted-AI/AIF360/blob/main/examples/demo_adversarial_debiasing.ipynb)\n",
        "\n",
        "Here we show how to learn and Adversarial Debiasing with the argumetn debias set to False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TgA5dwrdDycV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfb13b1-e70d-4361-d5b3-7ae0ea645b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 0.961256\n",
            "epoch 1; iter: 0; batch classifier loss: 0.784680\n",
            "epoch 2; iter: 0; batch classifier loss: 0.616059\n",
            "epoch 3; iter: 0; batch classifier loss: 0.507672\n",
            "epoch 4; iter: 0; batch classifier loss: 0.382953\n",
            "epoch 5; iter: 0; batch classifier loss: 0.326099\n",
            "epoch 6; iter: 0; batch classifier loss: 0.259677\n",
            "epoch 7; iter: 0; batch classifier loss: 0.219285\n",
            "epoch 8; iter: 0; batch classifier loss: 0.395851\n",
            "epoch 9; iter: 0; batch classifier loss: 0.341150\n",
            "epoch 10; iter: 0; batch classifier loss: 0.299453\n",
            "epoch 11; iter: 0; batch classifier loss: 0.302512\n",
            "epoch 12; iter: 0; batch classifier loss: 0.295940\n",
            "epoch 13; iter: 0; batch classifier loss: 0.314817\n",
            "epoch 14; iter: 0; batch classifier loss: 0.299420\n",
            "epoch 15; iter: 0; batch classifier loss: 0.262912\n",
            "epoch 16; iter: 0; batch classifier loss: 0.338670\n",
            "epoch 17; iter: 0; batch classifier loss: 0.273303\n",
            "epoch 18; iter: 0; batch classifier loss: 0.276795\n",
            "epoch 19; iter: 0; batch classifier loss: 0.253524\n",
            "epoch 20; iter: 0; batch classifier loss: 0.367428\n",
            "epoch 21; iter: 0; batch classifier loss: 0.172485\n",
            "epoch 22; iter: 0; batch classifier loss: 0.349494\n",
            "epoch 23; iter: 0; batch classifier loss: 0.378304\n",
            "epoch 24; iter: 0; batch classifier loss: 0.310849\n",
            "epoch 25; iter: 0; batch classifier loss: 0.329081\n",
            "epoch 26; iter: 0; batch classifier loss: 0.249102\n",
            "epoch 27; iter: 0; batch classifier loss: 0.261227\n",
            "epoch 28; iter: 0; batch classifier loss: 0.338689\n",
            "epoch 29; iter: 0; batch classifier loss: 0.298593\n",
            "epoch 30; iter: 0; batch classifier loss: 0.326583\n",
            "epoch 31; iter: 0; batch classifier loss: 0.329281\n",
            "epoch 32; iter: 0; batch classifier loss: 0.257389\n",
            "epoch 33; iter: 0; batch classifier loss: 0.346143\n",
            "epoch 34; iter: 0; batch classifier loss: 0.292558\n",
            "epoch 35; iter: 0; batch classifier loss: 0.255095\n",
            "epoch 36; iter: 0; batch classifier loss: 0.224831\n",
            "epoch 37; iter: 0; batch classifier loss: 0.203827\n",
            "epoch 38; iter: 0; batch classifier loss: 0.291387\n",
            "epoch 39; iter: 0; batch classifier loss: 0.285531\n",
            "epoch 40; iter: 0; batch classifier loss: 0.259872\n",
            "epoch 41; iter: 0; batch classifier loss: 0.219181\n",
            "epoch 42; iter: 0; batch classifier loss: 0.212457\n",
            "epoch 43; iter: 0; batch classifier loss: 0.162259\n",
            "epoch 44; iter: 0; batch classifier loss: 0.339833\n",
            "epoch 45; iter: 0; batch classifier loss: 0.234472\n",
            "epoch 46; iter: 0; batch classifier loss: 0.236712\n",
            "epoch 47; iter: 0; batch classifier loss: 0.279815\n",
            "epoch 48; iter: 0; batch classifier loss: 0.266735\n",
            "epoch 49; iter: 0; batch classifier loss: 0.339287\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7829f741cb30>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "plain_model = AdversarialDebiasing(\n",
        "    unprivileged_groups=[{'RACE': 0.0}],\n",
        "    privileged_groups=[{'RACE': 1.0}],\n",
        "    scope_name='plain_classifier',\n",
        "    debias=False,\n",
        "    sess=sess)\n",
        "\n",
        "plain_model.fit(dataset_orig_panel19_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8pgJF7gwDycV"
      },
      "outputs": [],
      "source": [
        "# Apply the plain model to train and val data\n",
        "dataset_nodebiasing_train = plain_model.predict(dataset_orig_panel19_train)\n",
        "dataset_nodebiasing_val = plain_model.predict(dataset_orig_panel19_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BPXEKWEyDycV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30352458-c768-4c85-b19f-2342f01455ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'base_rate_truth': np.float64(0.2175580473224301),\n",
              " 'statistical_parity_difference': np.float64(-0.1460238687255686),\n",
              " 'disparate_impact_ratio': 0.3558503689905921,\n",
              " 'base_rate_preds': np.float64(0.1686109053620597),\n",
              " 'equal_opportunity_difference': -0.17394215877176566,\n",
              " 'average_odds_difference': -0.11286350521421812,\n",
              " 'conditional_demographic_disparity': np.float64(-0.05102689124979527),\n",
              " 'smoothed_edf': np.float64(1.0332447921975951),\n",
              " 'df_bias_amplification': np.float64(0.3348973987581134),\n",
              " 'balanced_accuracy_score': np.float64(0.774212852295711)}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "get_metrics(\n",
        "    y_true = dataset_orig_panel19_train.labels[:,0],\n",
        "    y_pred= dataset_nodebiasing_train.labels[:,0],\n",
        "    prot_attr= dataset_orig_panel19_train.protected_attributes[:,0],\n",
        "    sample_weight= dataset_orig_panel19_train.instance_weights\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "l1sU9551DycV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90fdff9f-df9c-48fa-d63d-9d8ef6b88556"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'base_rate_truth': np.float64(0.2174745734278294),\n",
              " 'statistical_parity_difference': np.float64(-0.14202380676146711),\n",
              " 'disparate_impact_ratio': 0.33962520969373916,\n",
              " 'base_rate_preds': np.float64(0.15697349832188962),\n",
              " 'equal_opportunity_difference': -0.16485180145804762,\n",
              " 'average_odds_difference': -0.12116322898032325,\n",
              " 'conditional_demographic_disparity': np.float64(-0.047200169741383725),\n",
              " 'smoothed_edf': np.float64(1.0799123105241468),\n",
              " 'df_bias_amplification': np.float64(0.4043317012542853),\n",
              " 'balanced_accuracy_score': np.float64(0.69217492865539)}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "get_metrics(\n",
        "    y_true = dataset_orig_panel19_val.labels[:,0],\n",
        "    y_pred= dataset_nodebiasing_val.labels[:,0],\n",
        "    prot_attr= dataset_orig_panel19_val.protected_attributes[:,0],\n",
        "    sample_weight= dataset_orig_panel19_val.instance_weights\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vOxGv1r0DycV"
      },
      "outputs": [],
      "source": [
        "sess.close()\n",
        "tf.reset_default_graph()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6nSHAJBDycV"
      },
      "source": [
        "### Question 7: Redo the same (learn and Adversarial Debiasing) with the argument debias set to True\n",
        "\n",
        "Compare the metrics outputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbcOJi97DycV"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ7CV8NgDycV"
      },
      "source": [
        "### Question 8: Combine the Reweighing with the Adversarial Debiasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koiYRffwDycV"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE0pd5MkDycV"
      },
      "source": [
        "This in-processing approach does not seem compatible withe the Reweighing, has the df_bias_amplification is high and the disparate impact ratio is not improved by the use of the reweighing has pre-processing.\n",
        "Although very efficient on the fairness metrics of the dataset, the Reweighing is not convenient for every kind of machine learning algo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nashEEuDDycV"
      },
      "source": [
        "## Analysis of the influence of Reweighing\n",
        "\n",
        "### QUESTION 9 : Pour aller plus loin, étudier l'impact du Reweighing sur différents modèles notamment les arbres de décision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GKirdT6DycV"
      },
      "outputs": [],
      "source": [
        "print(\"TODO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTpxnNpaDycV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "TD_corr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}